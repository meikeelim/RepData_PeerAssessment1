x <- matrix(1:6, 2, 3)
x
add2<- function(x, y){
x + y
}
add2(3, 5)
add2(13, 1)
above <- function(x, n){
use <- x > n
x[use]
x <-1:20
above <- function(x, n){
use <- x > n
x[use]
}
above10<-function(x){
use <- x > 10
x[use]
}
}
above10<-function(x){
use <- x > 10
x[use]
}
x<-1:20
above(x,12)
above<-function(x, n) {
use <- x> n
x[use]
}
above(x,12)
columnmean <- function(y){
nc <-ncol(y)
means<-numeric(nc)
for (i in 1:nc) {
means[i] <- means(y[,i])
}
means
}
colmunmean(airquality)
columnmean(airquality)
columnmean <- function(y){
nc <-ncol(y)
means<-numeric(nc)
for (i in 1:nc) {
means[i] <- mean(y[,i])
}
means
}
columnmean(airquality)
columnmean <- function(y, removeNA = TRUE){
nc <-ncol(y)
means<-numeric(nc)
for (i in 1:nc) {
means[i] <- mean(y[,i], na.rm = removeNA)
}
means
}
columnmean(airquality)
getwd()
cube <- function(x, n) {
x^3
}
cube(3)
x <- 1:10
if (x > 5){}
if (x > 5){
x <- 0}
x <- 1:10
x
if (x>5){}
if (x>5){
x<-0
}
f<-function(x){
g<-function(y){y+z}
z<-4
x + g(x)
}
z<-10
f(3)
x <- 5
y <- if (x < 3) { }
y <- if (x < 3) {
NA
} else {10}
y
h <- function (x, y = NULL, d = 3){
z<-cbind(x,d)
if(!is.null(y))
z<-z+y
else
z<-z+f
}
search()
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile = "getdata1.csv")
dateDownloaded <- date()
dateDownloaded
getwd()
housingdata <- read.csv("getdata1.csv", sep=",", header = TRUE)
head(housingdata)
nrows(housingdata)
nrow(housingdata)
ncol(housingdata)
milprop <- housingdata[which(housingdata$VAL==24),]
milprop
nrow(milprop)
library("httr", lib.loc="~/R/win-library/3.1")
install.packages("httr")
library(httr)
?oauth_endpoints
?oauth_app
?browse
??browse
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("github", "66e76358a9e5af5c711c", "e09141a4ab6fdd1213316459f966752069417682")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
2
gtoken <- config(token = github_token)
install.packages("httpuv")
library(httpuv)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/rate_limit", gtoken)
stop_for_status(req)
content(req)
BROWSE("https://api.github.com/users/jtleek/repos",authenticate("Access Token","x-oauth-basic","basic"))
json1 = content(req)
json1
json2 = jsonlite::fromJSON(toJSON(json1))
?jsonlite
??jsonlite
?view()
?view
??view
library(jsonlite)
library
json2 = jsonlite::fromJSON(toJSON(json1))
json2
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
content(req)
library(jsonlite)
json1 = content(req)
json2 = jsonlite::fromJSON(toJSON(json1))
names(json2)
names(json1)
names(json2$html_url)
class(json2)
dim(json2)
json3 = json2["html_url", "created_at"]
json3
dim(json3)
json1[4]
names(json2)
head(json2)
datashare<-json2[which(json2$full_name=="jtleek/datasharing"),]
datashare
datashare[, 45]
install.packages("sqldf")
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile = "Quiz2.csv")
dateDownloaded <- date()
acs <- read.csv("Quiz2.csv", sep=",", header = TRUE)
head(acs)
library(sqldf)
sqldf("select * from acs")
head(acs)
sqldf("select pwgtp1 from acs where AGEP < 50")
sqldf("select * from acs where AGEP < 50")
unique(acs$AGEP)
sqldf("select distinct AGEP from acs")
sqldf("select unique AGEP from acs")
con = url("http://biostat.jhsph.edu/~jleek/contact.html")
htmlCode = readLines(con)
close(con)
htmlCode[10]
nchar(htmlCode[10])
sapply(htmlCode[c(10, 20, 30, 100)], nchar)
lapply(htmlCode[c(10, 20, 30, 100)], nchar)
?read.fwf
?read.fwf
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
download.file(fileUrl, destfile = "Quiz2Q5.for")
dateDownloaded <- date()
data <- read.fwf("Quiz2Q5.for", header = TRUE)
head(data)
data <- read.fwf("Quiz2Q5.for", skip=4, widths=c(12, 7,4, 9,4, 9,4, 9,4))
head(data)
data_csv <- read.csv("Quiz2Q5.for", header = TRUE)
head(data_csv)
?sum
sum4col <- sum(data[,4])
sum4col
sum4colb <- sum(data[,"V4"])
sum4colb
url1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
f1 <- file.path(getwd(), "GDP.csv")
download.file(url1, f1)
url2 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
f2 <- file.path(getwd(), "EDU.csv")
download.file(url2, f2)
##Read in the data
##skip the top 5 rows as they are not needed, and read in only 190 rows from 6th row
GDP <- read.csv("GDP.csv", header = FALSE, skip=5, nrows=190)
EDU <- read.csv("EDU.csv", sep=",", header = TRUE)
View(EDU)
View(GDP)
GDP <- GDP[X != ""]
GDP <- data.table(GDP)
?data.table
??data.table
?data.table()
setnames(GDP, c("X", "X.1", "X.3", "X.4"), c("CountryCode", "rankingGDP", "Long.Name", "gdp"))
GDP2 <- GDP[, c(1,2,4,5)] ##Extract Col with info
colnames(GDP2) <- c("CountryCode", "rankingGDP", "Long.Name", "gdp")
mergeData <- merge(GDP2, EDU, by=c("CountryCode"), all=TRUE)
sortbyGDP <- mergeData[order(-mergeData[,2]),]
sortbyGDP2 <- mergeData[order(-mergeData$rankingGDP),]
View(sortbyGDP2)
sortbyGDP[13,]
sortbyGDP[13,1:4]
View(sortbyGDP)
View(GDP2)
View(mergeData)
View(mergeData)
mergeData[, mean(rankingGDP, na.rm=TRUE), by=Income.Group]
mergeData[, mean(rankingGDP, na.rm=TRUE), by="Income.Group"]
mergeData[, mean(rankingGDP, na.rm=TRUE), by=Income.Group]
tapply(mergeData$rankingGDP, mergeData$Income.Group, mean)
?tapply
tapply(mergeData$rankingGDP, mergeData$Income.Group, mean, na.rm=TRUE)
x <- tapply(mergeData$rankingGDP, mergeData$Income.Group, mean, na.rm=TRUE)
x
q <- quantile(mergeData$rankingGDP, c(0.2, 0.4, 0.6, 0.8, 1))
breaks <- quantile(dt$rankingGDP, probs=seq(0, 1, 0.2), na.rm=TRUE)
breaks <- quantile(mergeData$rankingGDP, probs=seq(0, 1, 0.2), na.rm=TRUE)
view(breaks)
?view
??view
View(breaks)
quintFactor <- cut(mergeData$rankingGDP, breaks = breaks, labels = c("1st", "2nd", "3rd", "4th", "5th"))
View(quintFactor)
mergeData$quintile <- quintFactor
View(mergeData)
with(mergeData, table(quintile, Income.Group))
R. version. string
R.version.string
library(swirl)
swirl()
swirl()
bye()
install.packages("swirl")
install.packages("swirl")
library(swirl)
swirl()
install_from_swirl("Getting and Cleaning Data")
swirl()
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
cran <- tbl_df(mydf)
rm("mydf")
cran
?select
View(cran)
select(cran, ip_id, package, country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran, -time)
bye()
swirl()
library(swirl)
swirl()
-5:20
-(5:20)
View(cran)
select(cran, -(X:size))
filter(cran, package == "swirl")
filter(cran, r_version == "3.1.1", country == "US")
?Comparision
?Comparison
filter(cran, r_version <= "3.0.2", country == "IN")
filter(cran, country == "US", country == "IN")
filter(cran, country == "US"| country == "IN")
filter(cran, size > 100500 & r_os == "linux-gnu")
filter(cran, size > 100500, r_os == "linux-gnu")
is.na(c(3,5, NA, 10))
!is.na(c(3, 5, NA, 10))
filter(cran, !na.rm(r_version))
filter(cran, !is.na(r_version))
cran2<- select(cran, size:ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran2, ip_id, package, size)
cran3 <- select(cran, ip_id, package, size)
cran3
View(cran)
mutate(cran3, size_mb = size/2^20)
mutate(cran3, size_mb = size/2^20, size_gb = size_mb/2^10)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
library(dplyr)
cran <- tbl_df(mydf)
rm("mydf")
cran
?group_by()
?group_by
View(cran)
by_package <- group_by(cran, package)
by_package
summarize(by_package, mean(size))
?n
?n_distinct
pack_sum <- summarize(by_package, count = n() ,
unique = n_distinct(ip_id) ,
countries = n_distinct(country) ,
avg_bytes = mean(size))
submit()
submit()
pack_sum
quantile(packsum$count, probs = 0.99)
quantile(pack_sum$count, probs = 0.99)
top_counts <- filter(pack_sum, count > 679)
top_counts
head(top_counts, 20)
arrange(top_counts, desc(count))
quantile(pack_sum$unique, probs = 0.99)
top_unique <- filter(pack_sum, unique > 465)
top_unique
arrange(top_unique, desc(unique))
submit()
submit()
submit()
cran %>% select(ip_id, country, package, size) %>% print
submit()
submit()
submit()
submit()
```
---
title: "Reproducible Research: Peer Assessment 1"
author: "Maggie Lim"
output:
html_document:
keep_md: true
---
library(knitr)
knit2html(“document.Rmd”)"
browseURL(“document.html”)"
## Loading and preprocessing the data
```{r readdata, echo=TRUE}
unzip(zipfile="activity.zip")
data <- read.csv(file="activity.csv", header=TRUE)
#Removes missing values and store in new data frame
data.na.omit <- na.omit(data)
```
## What is mean total number of steps taken per day?
####Calculate total number of steps taken per day
```{r, echo=TRUE}
library(plyr)
totalsteps = ddply(data.na.omit, .(date), summarize, steps = sum(steps))
```
####Plot Histogram
```{r, echo=TRUE}
hist(totalsteps$steps, breaks=30,
main="Total Number of Steps Per Day",
xlab="No. of Steps", ylab="Frequency of days",
col="red")
```
####Mean total number of steps taken per day
```{r, echo=TRUE}
mean(totalsteps$steps)
```
####Median total number of steps taken per day
```{r, echo=TRUE}
median(totalsteps$steps)
```
## What is the average daily activity pattern?
####Calculate mean number of steps taken daily based on the 5min intervals and plot graph
```{r, echo=TRUE}
aveDaily = ddply(data.na.omit , .(interval), summarise, steps=mean(steps))
plot(aveDaily$interval, aveDaily$steps, type="l",
main="Ave No. of Steps Taken based on a Time Series Plot of 5min Intervals",
xlab="Minutes",
ylab="Average No. of Steps")
```
####The 5-minute interval which contains the maximum number of steps
```{r, echo=TRUE}
aveDaily[which.max(aveDaily$steps),]
```
## Imputing missing values
####Total number of missing values in the dataset
```{r, echo=TRUE}
sum(is.na(data))
```
####Replace NA with mean for that day and created a new data set
```{r, echo=TRUE}
data2 <- data
for (i in 1:nrow(data2)) {
if (is.na(data2$steps[i])) {
data2$steps[i] <- aveDaily$steps[which(data2$interval[i] == aveDaily$interval)]
}
}
```
####Calculate total number of steps taken per day for new data set and plot histogram
```{r, echo=TRUE}
totalsteps2 = ddply(data2, .(date), summarize, steps = sum(steps))
#Plot Histogram
hist(totalsteps2$steps, breaks=30,
main="Total Number of Steps Per Day",
xlab="No. of Steps", ylab="Frequency of days",
col="blue")
```
####Mean total number of steps taken per day
```{r, echo=TRUE}
mean(totalsteps2$steps)
```
####Median total number of steps taken per day
```{r, echo=TRUE}
median(totalsteps2$steps)
```
These mean and median do not differ significantly from the previous mean and median.
But imputing missing values to the data, the only impact was an increase in the total number of observations.
## Are there differences in activity patterns between weekdays and weekends?
#### Assigning weekends and weekdays
```{r, echo=TRUE}
Sys.setlocale("LC_TIME", "English")
data2$day <- weekdays(as.Date(data2$date))   ##Create a 'day' column
#Assigning weekends and weekdays
data2$day <- weekdays(data2$date)
for (i in 1:nrow(data2)) {
if (data2$day[i] %in% c("Saturday","Sunday")) { #if Saturday or Sunday, assign 'weekend'
data2$day[i]<-"weekend"
}
else{
data2$day[i]<-"weekday"                 #else 'weekday'
}
}
```
####Create time series plot for weekdays and weekends
```{r, echo=TRUE}
aveDaily2 <- ddply(data2, .(interval, day), summarise, steps=mean(steps))
library(lattice)
xyplot(steps ~ interval | day, data = aveDaily2,
layout = c(1, 2), type="l",
xlab = "Interval", ylab = "Number of steps")
```
There is higher activity in the first 1000mins duing the weekday compared to the weekend. However, activity after 1000min is higher per interval during the weekend compared to the weekday.
knit2html(“document.Rmd”)"
library(knitr)
knit2html(“document.Rmd”)"
browseURL(“document.html”)"
library(knitr)
knit2html(“document.Rmd”)
browseURL(“document.html”)
#####These mean and median do not differ significantly from the previous mean and median.
setwd("C:/Users/limm5/datasciencecoursera/RepData_PeerAssessment1")
